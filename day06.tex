% !TEX root=main.tex

\pagebreak
\section{Day 6}

\subsection{Matrix multiplication -- how-to}

Given a basis $\{e_1,\cdots,e_n\}$ for $U$, given $u\in U$ we have that
\begin{align*}
  ST(v) &= ST(\alpha_1 e_1 + \cdots + \alpha_n e_n)\\
  &= S(\alpha_1 T(e_1) + \cdots + \alpha_n T(e_n))\\
  &= \alpha_1 ST(e_1) + \cdots + \alpha_n ST(e_n),
\end{align*}

so we want our new matrix $C$ to have column vectors $ST(e_k)$ in the natural basis of $W$.

\begin{definition}
  The \emph{matrix product} $AB$ of matrices $A\in M_{m\times n}(\bbF)$ and $B\in M_{p\times m}(\bbF)$ is defined as the matrix whose $i$-th column vector is the product $BA_i$ of $B$ and the $i$-th column of $A$.
\end{definition}

We can quickly check that matrix multiplication defined in this way gracefully corresponds to function composition. ****

By this isomorphism, we get a few matrix multiplication facts `for free'.

\subsection{Inverse matrix}

\begin{definition}
  The matrix $A\in M_{n\times n}(\bbF)$ is \emph{invertible} if it maps $\bbF^n\to\bbF^n$ bijectively.
\end{definition}

\emph{Remark:} if $m\neq n$, then $A$ is not invertible because the map is either not injective or not surjective.

\begin{proposition}
  With the matrix $A$ corresponding to $T:V\to W$, $A$ is invertible iff $T$ is invertible.
\end{proposition}
\begin{proof}
  $\Longrightarrow$ If $A$ is invertible, then $T$ is bijective, and so by Theorem \ref{thm:bijimpinv} $T\inv$ exists.

  $\Longleftarrow$ If $T\inv$ exists, then we have that $TT\inv= I$ and $T\inv T=I$, so by letting $B$ be the matrix corresponding to $T\inv$, we have that $AB=BA=I$, so $A$ is invertible.
\end{proof}

\begin{theorem}
  The matrix $A\in M_{m\times n}(\bbF)$ is invertible iff its columns form a basis for $\bbF^n$.
\end{theorem}
\begin{proof}
  $A$ is invertible

  $\Leftrightarrow$ $A$ is bijective

  $\Leftrightarrow$ for any $y\in\bbF^n$ there is a unique $x\in\bbF^n$ with $Ax=y$

  $\Leftrightarrow$ for any $y\in\bbF^n$ there is a unique $x\in\bbF^n$ with $x_1A_1 + \cdots x_n A_n = y$

  $\Leftrightarrow$ the columns of $A$ form a basis for $\bbF^n$.
\end{proof}

\subsection{Change of basis}

Given a finite-dimensional vector space $V$ and a pair of bases $\{e_1,\cdots,e_n\}$ and $\{u_1,\cdots,u_n\}$, we have that for $v\in V$
\begin{align*}
  v &= v_1 e_1 + \cdots + v_n e_n\\
  v &= v'_1 u_1 + \cdots + v'_n u_n.
\end{align*}

The question is: \emph{how are these two representations related?}

First, we express $u_i\in V$ in terms of the basis $\{e_i\}$:
\begin{align*}
  u_1 &= a_{11} e_1 + \cdots + a_{n1} e_n\\
  u_2 &= a_{12} e_1 + \cdots + a_{n2} e_n\\
  \vdots\\
  u_n &=a_{1n} e_1 + \cdots + a_{nn} e_n
\end{align*}

and bring these coefficients into matrix form:
\begin{align*}
A = \begin{bmatrix}
  u_1 & \cdots & u_n
\end{bmatrix} =
\begin{bmatrix}
  a_{11} & \cdots & a_{1n}\\
  \vdots & & \vdots\\
  a_{n1} & \cdots & a_{nn}
\end{bmatrix}
\end{align*}

((\emph{Claim:} this matrix is invertible. This is clear by noticing that its columns form a basis for $\bbF^n$))

We also write $e_i\in V$ in terms of the basis $\{u_i\}$:
\begin{align*}
B = \begin{bmatrix}
  e_1 & \cdots & e_n
\end{bmatrix} =
\begin{bmatrix}
  b_{11} & \cdots & b_{1n}\\
  \vdots & & \vdots\\
  b_{n1} & \cdots & b_{nn}
\end{bmatrix}.
\end{align*}

\emph{Claim:} $B=A\inv$. Consider the action of $BA$ on any $e_i$ expressed in the $\{e_i\}$ basis (important!):
$BA(e_i)=BA\begin{bmatrix}0\\ \vdots \\ 1 \\\vdots \\ 0\end{bmatrix}
= B\begin{bmatrix}a_{1i} \\ \vdots \\ a_{ni}\end{bmatrix}
= B u_i
= e_i$.

To sum up; $A\inv$ expresses $\{e_i\}$ in the $\{u_i\}$ basis. We call $A$ the \emph{change of basis matrix}. So given $v\in V$, we have that $v_e\in\bbF^n$ is related to $v_u\in\bbF^n$ by
\begin{align*}
  v_e &= Av_u\\
  A\inv v_e &= v_u.
\end{align*}

This allows us to relate the matrices of linear transformations:

\begin{theorem}
  Given a finite-dimensional vector space $V$, with two distinct bases $\{e_1,\cdots,e_n\}$ and $\{u_1,\cdots,u_n\}$, let $T\in L(V,V)$. Let $A$ and $B$ represent this linear transformations w.r.t. the each of the bases.

  Their relation is $B=S\inv A S$, with $S$ the change of basis matrix.
\end{theorem}
\begin{proof}
  Given $v,w\in V$ such that $Tv=w$, consider their basis representations $v_e,v_u,w_e,w_u\in\bbF^n$. We have that $Av_e=w_e$ and $Bv_u=w_u$.

  We have that $v_u=S\inv v_e$, and $w_u=S\inv w_e$, so $Sv_u = v_e$ and $Sw_u=w_e$. Substitute into our previous equality to get $ASv_u=Sw_u$.

  Apply $S\inv$ on both sides to get $S\inv ASv_u = w_u$; hence $B=S\inv AS$.
\end{proof}

\begin{definition}
  Two matrices $A,B$ are \emph{similar} if $B=S\inv A S$ for some invertible matrix $S$.
\end{definition}

Hence two similar matrices represent the same linear transformation in different bases.

\subsection{Rank and nullity -- intro}

Given two vector spaces $V$ and $W$, consider $T\in L(V,W)$.

\begin{definition}
  The \emph{image} of $T$ is defined by $T(V)=\{w\in W\mid T(v)=w, v\in V\}$.

  The \emph{null space} of $T$ is defined by $n(T)=\{v\in V\mid T(v)=0\}$.
\end{definition}

\begin{lemma}
  $T(V)$ and $n(T)$ are subspaces of $W$ and $V$ respectively.
\end{lemma}

\begin{definition}
  The \emph{rank} of $T$ is defined by $\rank(T)=|\dim T(V)|$.

  The \emph{nullity} of $T$ is defined by $\nullity(T)=|\dim n(T)|$.
\end{definition}

As a convention, if $\nullity T=0$, we let $n(T)=\{0\}$.

For next class; the \emph{rank-nullity theorem}.
