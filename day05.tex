% !TEX root=main.tex

\pagebreak
\section{Day 5}


Now that we have explored the formalities of linear transformations, we are going to do something interesting with them. Namely, we will find a way to represent linear transformations in a way that makes computations easier.

\subsection{Canonical bijection to the base field}
\begin{theorem}[Important]
  Any finite-dimensional vector space $V$ over a a field $\bbF$ is isomorphic to $\bbF^{\dim V}$.
\end{theorem}
\begin{proof}
  Given some basis $\{e_1,\cdots,e_n\}$ of $V$, any vector $v\in V$ can be \emph{uniquely} written as $v=v_1e_1 + \cdots + v_ne_n$. Define $T:V\to\bbF^n$ as $T(v)=(v_1,\cdots,v_n)$.

  We must ask a few questions:
  \begin{enumerate}[1)]
    \item \emph{Is $T$ well defined?} ...

    \item \emph{Is $T$ bijective?} ...

    \item \emph{Is $T$ linear?} ...
  \end{enumerate}

  All this works because we can find a \emph{finite basis of $V$}!
\end{proof}


Let's see some examples;
\begin{enumerate}[{Ex }1)]
  \item $P_n(\bbR)$. An obvious basis is $\{1,x,x^2\cdots,x^n\}$, so that $p(x)=a_0+a_1x+\cdots+a_nx^n$ gets sent to $(a_0,\cdots,a_n)\in\bbR^{n+1}$.

  However, note that this representation depends on the basis! If we choose $\{1,1+x,1+x+x^2,\cdots\}$ as a basis (it is), our polynomial is represented with a different vector.

  \item Vectors in $\bbR^2$, unless otherwise noted, are represented with respect to the canonical basis $\{(1,0),(0,1)\}$. The same idea works with $\bbR^n$ in general.
\end{enumerate}


\subsection{Matrices}
We now introduce a seemingly unrelated object; the matrix\footnote{Matrices are a pain to typeset, so I will write them sparingly}.

\begin{definition}
  A \emph{matrix} $A$ of size $m\times n$ is a table of scalars in $\bbF$ with $m$ rows and $n$ columns, with each entry $a_{ij}\in\bbF$.
\end{definition}

Let $M_{m\times n}(\bbF)=\{m\times n$ matrices with entries in $\bbF\}$.

The first useful thing to notice is that every $m\times n$ matrix $A$ defines a mapping $\bbF^n\to\bbF^m$ as follows:

\[
Av = ()() n-vector = () m-vector
\]

This operation is equivalent to writing $Av$ as the linear combination of the column vectors of $A$, so $Av\in\bbF^m$.

We define matrix addition and scalar multiplication in the natural way, so that $M_{m\times n}(\bbF)$ becomes a vector space over $\bbF$.

If $m=n$, we say that $A$ is a \emph{square matrix}, and so $A$ maps $\bbF^n\to\bbF^n$.


\subsection{Matrix of a linear transformation}
Now that $L(V,W)$ and $M_{m\times n}(\bbF)$ are on the same footing, both being vector spaces with base field $\bbF$, we start figuring out their connection.

\begin{lemma}
  Any $T\in L(V,W)$ is uniquely determined by its action on any basis of $V$.
\end{lemma}
\begin{proof}
  Given $v\in V$, we can write $v=v_1e_1+\cdots+v_ne_n$ for some basis of $V$. Hence $T(v)=T(v_1e_1+\cdots+v_ne_n)=T(v_1e_1)+\cdots+T(v_ne_n)=v_1T(e_1)+\cdots+v_nT(e_n)$.
\end{proof}

So as long as we figure out how $T$ acts on the basis of $V$, we can compute $T(v)$ for any $v\in V$. Our next step is to relate linear transformations with matrices.

Given some linear transformation $T:V\to W$, we will find a matrix $A:\bbF^n\to\bbF^m$ with $n=\dim(V)$ and $m=\dim(W)$. First let $\{e_1,\cdots,e_n\}$ be a basis of $V$ and $\{u_1, \cdots, u_m\}$ a basis of $W$. Define $a_k=T(e_k)$, i.e. the action of $T$ on the basis of $V$. We know that $T(v)=v_1T(e_1) + \cdots + v_nT(e_n) = v_1a_1 + \cdots + v_n a_n$, so any $T(v)$ is a linear combination of $\{a_1,\cdots,a_n\}$. We shall express each $a_k\in W$ in terms of its natural basis in $W$, $\{u_1,\cdots,u_m\}$, so $a_k=a_{1k}u_1 + \cdots + a_{mk}u_m$ for some coefficients $a_{ik}$.

We now put these column vectors in a matrix:
\begin{align*}
  A &= \begin{bmatrix}
    T(e_1) & T(e_2) & \cdots & T(e_n)
  \end{bmatrix}\\
  &= \begin{bmatrix}
    a_1 & a_2 & \cdots & a_n
  \end{bmatrix}\\
  &= \begin{bmatrix}
    a_{11} & \cdots & a_{1n}\\
    a_{21} & \cdots & a_{2n}\\
    \vdots & \cdots & \vdots\\
    a_{m1} & \cdots & a_{mn}
  \end{bmatrix}.
\end{align*}

Now, for every $v\in V$ represented by as an $n$-vector in $\bbF^n$ by the natural basis of $V$, $\{e_1, \cdots, e_n\}$, we compute the matrix-vector product with $A$:
\begin{align*}
  A\begin{bmatrix}
    v_1 \\ v_2 \\ \vdots \\ v_n
  \end{bmatrix} &= \begin{bmatrix}
    T(e_1) & \cdots & T(e_n)
  \end{bmatrix} \begin{bmatrix}
    v_1 \\ v_2 \\ \vdots \\ v_n
  \end{bmatrix}\\
  &= v_1 T(e_1) + \cdots + v_n T(e_n)\\
  &= T(v_1 e_1 + \cdots + v_n e_n) =T(v).
\end{align*}

Hence the matrix $A$ of coefficients faithfully represents the linear transformation $T$. We shall refer to matrices and their linear transformations as essentially the same thing from now on:

\begin{definition}
  The \emph{matrix of a linear transformation} $T\in L(V,W)$ with respect to a basis $\{e_1,\cdots,e_n\}$ of $V$ and $\{u_1,\cdots,u_m\}$ is defined by
  \[
    A = \begin{bmatrix}
      T(e_1) & \cdots & T(e_n)
    \end{bmatrix},
  \]
  with each $T(e_k)$ in the natural basis of $W$.
\end{definition}

We have found an isomorphism (details later) between the space of all linear transformations $V\to W$ and matrices $m\times n$ with coefficients in $\bbF$; $L(V,W)\simeq M_{m\times n}(\bbF)$.

\emph{Note;} this isomorphism is \emph{basis-dependent!} It's the tradeoff incurred by computability -- the moment we bring a linear transformation (an abstract object, an \emph{action}) to a numerical expression, a specific basis is required.

Some examples of matrices for previously explored linear transformations;
\begin{enumerate}[(A)]
  \item \emph{Rotation on the plane}: $R_\theta:\bbR^2\to\bbR^2$.

  We simply compute by trigonometry $R_\theta((1,0))$ and $R_\theta((0,1))$, and we get the matrix
  \[R_\theta = \begin{bmatrix}
    \cos\theta & -\sin\theta\\
    \sin\theta & \cos\theta
  \end{bmatrix}.\]

  To find the transformation matrix in a different basis, we need \emph{two} steps: first compute the action of $R_\theta$ on the new basis, and then express the result in the new basis, since $R_\theta(u_k)$ expresses the action in the canonical basis of $\bbR^2$!

  \item \emph{Reflections:} $T_l:\bbR^2\to\bbR^2$.

  ...

  \item \emph{Projections:} $P_l:\bbR^2\to\bbR^2$.

  ...
\end{enumerate}

\subsection{Matrix multiplication -- motivation}

The isomorphism between matrices and linear transformations is not complete; what matrix operation corresponds to function composition?

Given linear transformations $T:U\to V$ and $S:V\to W$, and their corresponding matrices $A:\bbF^n\to\bbF^m$, $B:\bbF^m\to\bbF^p$ we want to find a matrix $C:\bbF^n\to\bbF^p$ naturally corresponding to the composition $ST:U\to W$.

The matrix operation that satisfies this is \emph{matrix multiplication.}
