% !TEX root=main.tex

\pagebreak
\section{Day 2}

\subsection{Linear Combinations and Span}

Closure of a vector space $V$ under vector addition and scalar multiplication can be naturally generalized as follows:

\begin{definition}
  Let $A=\{a_1,\cdots,c_n\}\subset V$. A \emph{linear combination} of $A$ is a sum of the form
  \[
    \lambda_1 a_1 + \cdots + \lambda_n a_n
  \]
  for $\lambda_i\in\bbF$.
\end{definition}

We now verify that linear combinations behave nicely:
\begin{proposition}
  Let $S$ be a subspace of $V$ and $A=\{a_1,\cdots,a_n\}\subset S$. Then every linear combination of $A$ is in $S$. In other words; subspaces are closed under linear combinations.
\end{proposition}
\begin{proof}
  Induction on $n$. For the base case $n=1$; $\lambda_1 a_1\in S$ by closure of scalar multiplication.

  For the inductive step; assume the proposition works for some $m$, and consider $m+1$. Consider any linear combination $\lambda_1 a_1 + \cdots \lambda_m a_m + \lambda_{m+1} a_{m+1} = (\lambda_1 a_1 + \cdots + \lambda_n a_n) + (\lambda_{m+1} a_{m+1})$. The first term of the sum is in $S$ by assumption, and the second term is in $S$ by closure of scalar multiplication. Since $S$ is closed under vector addition, the sum of these two terms is in $S$, hence the proposition holds for $m+1$.
\end{proof}

\begin{definition}
  Given $A=\{a_1,\cdots,a_n\}\subset V$, the \emph{span of $A$} is defined as
  \begin{align*}
    \sspan(A) &= \{\lambda_1 a_1+\cdots+\lambda_n a_n\}\\
      &= \{\text{all linear combinations of } A\}.
  \end{align*}
\end{definition}

\begin{enumerate}[Ex. A)]
\item The plane $P=\{\alpha v_1 + \beta v_2 \mid \alpha,\beta\in\bbR\}\subset\bbR^3$ as defined previously, for adequately-chosen $v_1,v_2\in\bbR^3$. Note that $P=\sspan(v_1,v_2)$; we say that $P$ is \emph{spanned} or \emph{generated} by $\{v_1,v_2\}$.
\end{enumerate}

\begin{question}
  Can $P$ be written as $P=\sspan(v_1,v_2,v_3)$ for some $v_1,v_2,v_3\in\bbR^3$?
\end{question}
\begin{answer}
  If $v_3=\lambda_1 v_1 + \lambda_2 v_2$, yes. Consider any linear combination of these three vectors;
  \begin{align*}
    v'=\alpha v_1 + \beta v_2 + \gamma v_3 &= (\alpha + \gamma\lambda_1)v_1 + (\beta + \gamma\lambda_2)v_2\\
    &= a v_1 + b v_2
  \end{align*}
  with $a,b\in\bbR$, so $v'\in\sspan(v_1,v_2)=P$. In other words, any $v_3$ of this form is unnecessary to span $P$, because $\sspan(v_1,v_2,v_3)=\sspan(v_1,v_2)$.
\end{answer}

\subsection{Linear Dependence and Independence}

Hence it is convenient, for some vector space (or subspace), to find the smallest set of vectors which span it. This motivates the following definition:

\begin{definition}
  A finite collection of vectors $A=\{a_1,\cdots,a_n\}\subset V$ is \emph{linearly dependent} if one of the $a_i\in A$ can be written as a linear combination of the others:
  \[
    a_i = \sum_{1\leq j \leq n, j\neq i} \lambda_j a_j
  \]
  for $\lambda_j\in\bbF$.

  Otherwise, we say that $A$ is \emph{linearly independent}.
\end{definition}


The following proposition provides an equivalent definition for linear dependency.

\begin{proposition}\label{prop:lindepeq}
  A collection $A=\{a_1,\cdots,a_n\}\subset V$ is linearly dependent iff there exists a collection of scalars $\lambda_1,\cdots,\lambda_n\in\bbF$, at least one of them non-zero, such that
  \[
    \lambda_1 a_1 + \cdots + \lambda_n a_n = 0.
  \]
\end{proposition}
\begin{proof}
  $\implies$ Assume that $A$ is linearly dependent. Hence at least one $a_i$ can be written as $a_i=\sum_{1\leq j \leq n, j\neq i} \lambda_j a_j$. Subtracting $a_i$ from both sides we get $0=\lambda_1 a_1 + \cdots (-1) a_i + \cdots + \lambda_n a_n$. By setting $\lambda_i = -1$, the first direction is done.

  $\impliedby$ Assume $0=\sum_{1\leq i\leq n} \lambda_i a_i$. Since there is some non-zero $\lambda_j$, subtract $\lambda_j a_j$ from both sides and divide over $-\lambda_j$ to get
  \[
    a_j = \sum_{1\leq i\leq n,i\neq j} \left(\frac{-\lambda_i}{\lambda_j}\right) a_i.
  \]
  This satisfies the definition of linear dependence.
\end{proof}

\begin{proposition}\label{prop:linindep}
  A collection $A=\{a_1,\cdots,a_n\}\subset V$ is linearly independent iff $\lambda_1 a_1 + \cdots + \lambda_n = 0$ implies that $\lambda_i = 0$ for $1\leq i \leq n$.
\end{proposition}
\begin{proof}
  This is the negation of Proposition \ref{prop:lindepeq}.
\end{proof}

Some examples of linear dependence and independence:

\begin{enumerate}[Ex. A)]
\item Any two vectors $a_1,a_2\in V$ are linearly dependent iff $a_1=\lambda a_2$.

\item In $\bbR^3$, consider the vectors $v_1=(1,0,1)$, $v_2=(0,1,0)$ and $v_3=(0,1,1)$. The only solutions for the equation $\lambda_1 v_1+\lambda_2 v_2 + \lambda_3 v_3=0$ (by separating into three component-wise equations) is $\lambda_1=\lambda_2=\lambda_3=0$. So by Proposition \ref{prop:linindep}, these three vectors are linearly independent.

\emph{Remark:} we will develop \emph{Gaussian elimination} to systematically find whether a set of vectors is linearly independent or not.

\item For any collection $A=\{a_1,\cdots,a_n\}\subset V$ with some $a_j=0$, let $\lambda_j=1$ and all other $\lambda_i=0$. Then $\sum_{1\leq i\leq n} \lambda_i a_i=0$, but not all $\lambda_i$ are zero, so $A$ is linearly dependent.

\item Take $a=(1,0)$, $b=(0,1)$ and $c=(c_1,c_2)$ as elements of $\bbR^3$. Since $c=c_1 a + c_2 b$, these three vectors are not linearly independent.

\item Given some linearly dependent set $A=\{a_1,\cdots,a_n\}\subset V$ and any set $A'=\{a_1,\cdots,a_m\}\subset V$ with $A\subset A'$ (so $m>n$), consider some linear combination $\sum_{1\leq i \leq m} \lambda_i a_i$ .... RELABEL, or simplify somehow.
\end{enumerate}

\subsection{Basis and Dimension}
\begin{definition}
  A \emph{basis} of a vector space $V$ is a finite set $E=\{e_1,\cdots,e_n\}\subset V$ such that:
  \begin{enumerate}[1)]
    \item $E$ is linearly independent.
    \item $\sspan(E)=V$.
  \end{enumerate}
\end{definition}

\begin{proposition}
  If $E=\{e_1,\cdots,e_n\}$ is a basis of $V$, then every $v\in V$ can be uniquely written as a linear combination of $E$.
\end{proposition}
\begin{proof}
  Given any $v\in V$, since $\sspan(E)=V$, then $v=v_1 e_1 + \cdots + v_n e_n$ for $v_i\in\bbF$. Assume that also $v=w_1 e_1 + \cdots + w_n e_n$ for $w_i\in\bbF$ with $w_i\neq v_i$. Subtract both equalities to get $0 = (v_1-w_1)e_1 + \cdots + (v_n-w_n)e_n$. Since $E$ is linearly independent, this implies that $v_i-w_i=0$, so $v_i=w_i$, a contradiction. Hence the representation of $v$ as a linear combination of $E$ is unique.
\end{proof}

\begin{remark*}
  A vector space might have many bases. For example; $\{(1,0),(0,1)\}$ and $\{(1,1),(0,2)\}$ are both bases of $\bbR^2$.
\end{remark*}

\begin{theorem}[Important]\label{thm:basedep}
  If a vector space $V$ has bases $E=\{e_1,\cdots, e_n\}$ and $U=\{u_1,\cdots, u_m\}$, then $n=m$.
\end{theorem}

In other words; the cardinality of a basis of $V$ is uniquely determined by $V$. This fact motivates the following definition:

\begin{definition}
  If a vector space $V$ has a finite basis $E$, then it is \emph{finite-dimensional}, with dimension defined as $\dim(V)=|E|$.

  Else, $V$ is \emph{infinite-dimensional}.
\end{definition}

\begin{proposition}\label{prop:subspacedim}
  If $W$ is a subspace of $V$ and $\dim(V)$ is finite, then $\dim(W)\leq\dim(V)$.
\end{proposition}
\begin{proof}
  Take a basis $E=\{e_1,\cdots,e_n\}$ of $V$. Since $W\subset V$, any $w\in W$ can be represented as a linear combination of $E$; hence any basis of $W$ must have no more than $n$ elements... else?? **
\end{proof}

\begin{question}
  Example of a $\infty$-dimensional vector space?
\end{question}
\begin{answer}
  Consider $P(\bbR)$, and assume that $\dim(P(\bbR))=k<\infty$. Given some $M>k$, let $P_M(\bbR)=\{M$-degree polynomials in $\bbR\}$. It is clear that $P_M(\bbR)\subset P(\bbR)$. Consider the set $\{x^i\}_{i=0}^M$; this is a basis of $P_M(\bbR)$, so $\dim(P_M(\bbR))=M+1>k$, which contradicts the fact that, by Proposition \ref{prop:subspacedim}, $\dim(P_M(\bbR))\leq \dim(P(\bbR))$. Hence $P(\bbR)$ is infinite dimensional.
\end{answer}
